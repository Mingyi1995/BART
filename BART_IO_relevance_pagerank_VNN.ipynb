{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc05a381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from joblib import * \n",
    "from sklearn.metrics import r2_score,mean_absolute_error,mean_absolute_percentage_error,mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "# import geopandas as gpd\n",
    "# from geopy.distance import distance,geodesic\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c55da5c",
   "metadata": {},
   "source": [
    "# different methods, all tested on 09-26 to 10-02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0988005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_MAPE(v, v_, axis=None):\n",
    "    '''\n",
    "    Mean absolute percentage error.\n",
    "    :param v: np.ndarray or int, ground truth.\n",
    "    :param v_: np.ndarray or int, prediction.\n",
    "    :param axis: axis to do calculation.\n",
    "    :return: int, MAPE averages on all elements of input.\n",
    "    '''\n",
    "    mask = (v == 0)\n",
    "    percentage = np.abs(v_ - v) / np.abs(v)\n",
    "    if np.any(mask):\n",
    "        masked_array = np.ma.masked_array(percentage, mask=mask)  # mask the dividing-zero as invalid\n",
    "        result = masked_array.mean(axis=axis)\n",
    "        if isinstance(result, np.ma.MaskedArray):\n",
    "            return result.filled(np.nan)\n",
    "        else:\n",
    "            return result\n",
    "    return np.mean(percentage, axis).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80b6c68b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bf9423b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class relevance_VNN(nn.Module):\n",
    "    def __init__(self,node_feat,input_size,n_feature=128):\n",
    "        super(relevance_VNN, self).__init__()\n",
    "        \n",
    "        self.relevance = nn.Sequential(\n",
    "        nn.Linear(node_feat.shape[1], n_feature),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Linear(n_feature,n_feature),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Linear(n_feature,n_feature),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Linear(n_feature,n_feature),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Linear(n_feature,input_size),\n",
    "        )\n",
    "        \n",
    "        self.weight = nn.Sequential(\n",
    "        nn.Linear(input_size, n_feature),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Linear(n_feature,n_feature),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Linear(n_feature,n_feature),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Linear(n_feature,n_feature),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Linear(n_feature,2),\n",
    "        )\n",
    " \n",
    "    def forward(self,x,node_feat):\n",
    "#         x = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "        att = self.relevance(node_feat)\n",
    "        \n",
    "#         print(att.shape,x.shape)\n",
    "#         relevance * flows\n",
    "        flow = att.mul(x)\n",
    "\n",
    "        flow = self.weight(x)\n",
    "        return flow\n",
    "\n",
    "\n",
    "def get_loss_and_metrics(model,node_feat, data, target,criterion, device):\n",
    "  # Implement forward pass and loss calculation for one batch.\n",
    "  # Remember to move the batch to device.\n",
    "  # \n",
    "  # Return a tuple:\n",
    "  # - loss for the batch (Tensor)\n",
    "  # - number of correctly classified examples in the batch (Tensor)\n",
    "#     data, target = batch[0], batch[1]\n",
    "#     print(data.shape)\n",
    "    data = torch.tensor(data, dtype=torch.float32)\n",
    "    target = torch.tensor(target, dtype=torch.float32)\n",
    "    node_feat = torch.tensor(node_feat, dtype=torch.float32)\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(data,node_feat)\n",
    "#     print(pred,target)\n",
    "    loss = criterion(pred, target)\n",
    "    \n",
    "    \n",
    "    return (pred,target,loss)\n",
    "    \n",
    "def step(loss, optimizer):\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11865a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Supports two versions – random walker with a “teleport” home \\nor the “stopping” one – starting in its residence with 1-alpha chance of stopping at each step. \\nBoth work generally fine to represent the node’s function in a network through. A is an adjacency matrix'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def residential_page_rank_embedding(A, alpha=0.85, personalized = 'teleport'): \n",
    "# get node embedding vector as probabilities of visiting other nodes with a \n",
    "# network random walk and an 1-alpha percent home teleport home\n",
    "    w = A.sum(axis = 1).reshape(-1,1)\n",
    "    n = A.shape[0]\n",
    "    A = (A + (w == 0)) / (w + n * (w == 0))\n",
    "    AI = np.linalg.inv(np.eye(n) - A * alpha)\n",
    "    X = (1 - alpha) * AI\n",
    "    if personalized != 'teleport':\n",
    "        X = np.matmul(X, A)\n",
    "    return X\n",
    "\n",
    " \n",
    "\n",
    "'''Supports two versions – random walker with a “teleport” home \n",
    "or the “stopping” one – starting in its residence with 1-alpha chance of stopping at each step. \n",
    "Both work generally fine to represent the node’s function in a network through. A is an adjacency matrix'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026ed6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:441 training batch:52 total train loss:0.044 validation_mae:32.498 validation R2: 0.221 validation MAPE: 0.76:  70%|▋| 35/5"
     ]
    }
   ],
   "source": [
    "nodes_dist = pd.read_csv('nodes_dist.csv')\n",
    "G = nx.read_gpickle('graph.pickle')\n",
    "od = pd.read_csv('inoutwide.csv')\n",
    "pbar = tqdm(list(G.nodes))\n",
    "count = 0\n",
    "\n",
    "G = nx.read_gpickle('graph.pickle')\n",
    "A = nx.adjacency_matrix(G)\n",
    "pagerank = residential_page_rank_embedding(A, alpha=0.85, personalized = 'stopping')\n",
    "pagerank = pd.DataFrame(pagerank)\n",
    "pagerank.columns = list(G.nodes)\n",
    "pagerank['from'] = list(G.nodes)\n",
    "\n",
    "for station in pbar:\n",
    "    \n",
    "#     neighbors = list(nx.ego_graph(G, radius=1, n=station, distance='weight', center=False).nodes)\n",
    "    neighbors = nodes_dist.loc[(nodes_dist['o']==station)&\\\n",
    "                               (nodes_dist['dist']<1)&\\\n",
    "                               (nodes_dist['o']!=nodes_dist['d'])]['d'].values.tolist()\n",
    "    flows = ['incoming_flow-'+station]+\\\n",
    "            ['incoming_flow-'+i for i in neighbors]+['outgoing_flow-'+station]+\\\n",
    "                      ['outgoing_flow-'+i for i in neighbors]\n",
    "\n",
    "    fts = od[['Date','Hour']+flows]\n",
    "    lags = list(range(1,24))+list(np.array(list(range(2,8)))*24)+list(np.array(list(range(2,8)))*24*7)\n",
    "    for lag in lags:\n",
    "        temp = fts[flows].shift(lag)\n",
    "        temp.columns = [i+'-lag-'+str(lag) for i in flows]\n",
    "        fts = pd.concat([fts,temp],axis=1)\n",
    "    fts = fts.dropna()\n",
    "    fts = fts.drop(columns=['Date','Hour'])\n",
    "#     print('here')\n",
    "    \n",
    "    node_feat_raw = pd.DataFrame()\n",
    "    node_feat_raw['neighbor'] = [station]+neighbors\n",
    "    node_feat_raw['degree'] = node_feat_raw.apply(lambda x:nx.shortest_path_length(G,station,x['neighbor']),axis=1)\n",
    "    node_feat_raw['path_distance'] = node_feat_raw.apply(lambda x:nx.dijkstra_path_length(G,station,x['neighbor']),\n",
    "                                                         axis=1)\n",
    "    temp = pagerank.loc[pagerank['from']==station].drop(columns='from')\n",
    "    temp.columns = ['page_rank_station' + str(i) for i in range(50)]\n",
    "    node_feat_raw = pd.concat([node_feat_raw,temp],axis=1)\n",
    "    for nei in neighbors:\n",
    "        temp = pagerank.loc[pagerank['from']==nei].drop(columns='from')\n",
    "        temp.columns = ['page_rank_neighbor'+nei + str(i) for i in range(50)]\n",
    "        node_feat_raw = pd.concat([node_feat_raw,temp],axis=1)\n",
    "    # repeat itself to match number of lags\n",
    "    node_feat_raw = node_feat_raw.append([node_feat_raw]*(len(lags)*2*(len(neighbors)+1)-1),ignore_index=True)\n",
    "    \n",
    "    del node_feat_raw['neighbor']\n",
    "    node_feat_raw = node_feat_raw.to_numpy().flatten()\n",
    "    \n",
    "    node_feat_raw = node_feat_raw.reshape(1,node_feat_raw.shape[0],)\n",
    "    # node_feat_raw in wide form, and it is the same for each time point, for each lag\n",
    "    \n",
    "    # len(lags) is number of lags, 2 is bidirectional\n",
    "    # remove flows at time t\n",
    "    fts_train = fts.iloc[:-24*7,(len(neighbors)+1)*2:].values\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(fts_train)\n",
    "    fts_train = torch.tensor(scaler.transform(fts_train))\n",
    "\n",
    "    y = fts[['incoming_flow-'+station,'outgoing_flow-'+station]]\n",
    "    y_train = y.iloc[:-24*7,:].values\n",
    "    y_scaler = StandardScaler()\n",
    "    y_scaler.fit(y_train)\n",
    "    y_train = y_scaler.transform(y_train)\n",
    "    y_train = torch.tensor(y_train)\n",
    "    train_dataset = torch.utils.data.TensorDataset(fts_train,y_train)\n",
    " \n",
    "\n",
    "    fts_val = torch.tensor(scaler.transform(fts.iloc[-24*7:,(len(neighbors)+1)*2:].values)) \n",
    "    y_val = y.iloc[-24*7:,:].values\n",
    "    y_val = torch.tensor(y_val)    \n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "    N_EPOCHS = 501\n",
    "    BATCH_SIZE = 64\n",
    "#     print('here')\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                                                   num_workers=0)\n",
    "\n",
    "    node_feat = np.repeat(node_feat_raw,BATCH_SIZE,axis=0)\n",
    "    node_feat = torch.tensor(node_feat, dtype=torch.float32).to(device)\n",
    "#     break\n",
    "    model = relevance_VNN(node_feat,input_size=(1+len(neighbors))*len(lags)*2)\n",
    "    model = model.to(device)\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01) \n",
    "#     qbar = tqdm(range(N_EPOCHS), leave=False)\n",
    "    validation_mae,validation_r2,validation_mape = 999,999,999\n",
    "    for i in range(N_EPOCHS): \n",
    "\n",
    "        total_train_loss = 0.0\n",
    "        model.train()\n",
    "        training_batch_count = 1\n",
    "        for batch in train_dataloader:\n",
    "            \n",
    "            node_feat = np.repeat(node_feat_raw,len(batch[0]),axis=0)\n",
    "            node_feat = torch.tensor(node_feat, dtype=torch.float32).to(device)\n",
    "            y_train,y_true,loss = get_loss_and_metrics(model,node_feat, batch[0],batch[1], criterion, device)\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            mean_train_loss = total_train_loss / training_batch_count\n",
    "            pbar.set_description('epoch:' + str(i) +\\\n",
    "                ' training batch:' + str(training_batch_count) +\\\n",
    "                ' total train loss:'+ str(round(mean_train_loss,3))+ \\\n",
    "                 ' validation_mae:'+ str(round(validation_mae,3)) +\\\n",
    "                ' validation R2: '+ str(round(validation_r2,3)) +\\\n",
    "                ' validation MAPE: '+ str(round(validation_mape,3)))\n",
    "            training_batch_count += 1\n",
    "            \n",
    "            step(loss,optimizer)\n",
    "        \n",
    "        if i%50 == 0 and i >=50:\n",
    "#         if i >= 0:\n",
    "            with torch.no_grad(): \n",
    "                node_feat = np.repeat(node_feat_raw,len(fts_val),axis=0)\n",
    "                node_feat = torch.tensor(node_feat, dtype=torch.float32).to(device)\n",
    "                y_pred,y_true,loss = get_loss_and_metrics(model,node_feat, fts_val, y_val, criterion, device)\n",
    "                y_pred_val,y_true_val = y_pred.cpu().numpy(),y_true.cpu().numpy()\n",
    "                y_pred_val = y_scaler.inverse_transform(y_pred_val)\n",
    "\n",
    "                validation_mae = mean_absolute_error(y_true_val,y_pred_val)\n",
    "                validation_r2 = r2_score(y_true_val,y_pred_val)\n",
    "                validation_mape = masked_MAPE(y_true_val,y_pred_val)\n",
    "                pbar.set_description('epoch:' + str(i) +\\\n",
    "                ' training batch:' + str(training_batch_count) +\\\n",
    "                ' total train loss:'+ str(round(mean_train_loss,3))+ \\\n",
    "                 ' validation_mae:'+ str(round(validation_mae,3)) +\\\n",
    "                ' validation R2: '+ str(round(validation_r2,3)) +\\\n",
    "                ' validation MAPE: '+ str(round(validation_mape,3)))\n",
    "                        \n",
    "                \n",
    "                \n",
    "    if count == 0: \n",
    "        y_true_total = y_true_val\n",
    "        y_pred_total = y_pred_val\n",
    "    else:\n",
    "        y_true_total = np.concatenate([y_true_total,y_true_val])\n",
    "        y_pred_total = np.concatenate([y_pred_total,y_pred_val])\n",
    "    count += 1\n",
    "    torch.save(model.state_dict(), 'pagerank_VNN/'+station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed68a5d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "95e52e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9731576795262274\n",
      "17.390324\n",
      "36.29152\n",
      "0.16377626\n",
      "0.4908718337347174\n"
     ]
    }
   ],
   "source": [
    "# y_true,y_pred = y_true.cpu().numpy(),y_pred.cpu().numpy()\n",
    "print(r2_score(y_true_total,y_pred_total))\n",
    "print(mean_absolute_error(y_true_total,y_pred_total))\n",
    "print(mean_squared_error(y_true_total,y_pred_total,squared=False))\n",
    "print(mean_squared_error(y_true_total,y_pred_total,squared=False)/np.mean(np.std(y_true_total)))\n",
    "\n",
    "print(masked_MAPE(y_true_total,y_pred_total))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b72142",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
