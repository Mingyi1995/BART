{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc05a381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from joblib import * \n",
    "from sklearn.metrics import r2_score,mean_absolute_error,mean_absolute_percentage_error,mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "# import geopandas as gpd\n",
    "# from geopy.distance import distance,geodesic\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import acf, pacf,adfuller, kpss,range_unit_root_test\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c55da5c",
   "metadata": {},
   "source": [
    "# different methods, all tested on 09-26 to 10-02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0988005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_MAPE(v, v_, axis=None):\n",
    "    '''\n",
    "    Mean absolute percentage error.\n",
    "    :param v: np.ndarray or int, ground truth.\n",
    "    :param v_: np.ndarray or int, prediction.\n",
    "    :param axis: axis to do calculation.\n",
    "    :return: int, MAPE averages on all elements of input.\n",
    "    '''\n",
    "    mask = (v == 0)\n",
    "    percentage = np.abs(v_ - v) / np.abs(v)\n",
    "    if np.any(mask):\n",
    "        masked_array = np.ma.masked_array(percentage, mask=mask)  # mask the dividing-zero as invalid\n",
    "        result = masked_array.mean(axis=axis)\n",
    "        if isinstance(result, np.ma.MaskedArray):\n",
    "            return result.filled(np.nan)\n",
    "        else:\n",
    "            return result\n",
    "    return np.mean(percentage, axis).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80b6c68b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "c01d2a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class relevance_VNN(nn.Module):\n",
    "    def __init__(self,input_size, n_feature, output_size):\n",
    "        super(relevance_VNN, self).__init__()\n",
    "        \n",
    "        self.relevance = nn.Sequential(\n",
    "        nn.Linear(input_size, n_feature),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Linear(n_feature,input_size),\n",
    "        )\n",
    "        \n",
    "        self.weight = nn.Sequential(\n",
    "        nn.Linear(input_size, n_feature),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Linear(n_feature,n_feature),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Linear(n_feature,n_feature),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Linear(n_feature,2),\n",
    "        )\n",
    " \n",
    "    def forward(self, x,verbose=False):\n",
    "        att = self.relevance(x)\n",
    "        # relevance * flows\n",
    "        flow = att.mul(x)\n",
    "#         print(att.shape,x.shape,flow.shape)\n",
    "        # reshape to get weighted avaerge from all neighbors at each time lag\n",
    "        flow = self.weight(x)\n",
    "        flow = torch.mean(flow,0)   \n",
    "        return flow\n",
    "\n",
    "def get_loss_and_metrics(model, batch,criterion, device):\n",
    "  # Implement forward pass and loss calculation for one batch.\n",
    "  # Remember to move the batch to device.\n",
    "  # \n",
    "  # Return a tuple:\n",
    "  # - loss for the batch (Tensor)\n",
    "  # - number of correctly classified examples in the batch (Tensor)\n",
    "    data, target = batch[0], batch[1]\n",
    "#     print(data.shape)\n",
    "    target = torch.mean(target,0)\n",
    "    data = torch.tensor(data, dtype=torch.float32)\n",
    "    target = torch.tensor(target, dtype=torch.float32)\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(data)\n",
    "#     print(pred,target)\n",
    "    loss = criterion(pred, target)\n",
    "    \n",
    "    \n",
    "    return (pred,target,loss)\n",
    "    \n",
    "def step(loss, optimizer):\n",
    "  # Implement backward pass and update.\n",
    "\n",
    "  # TODO\n",
    "    loss = loss\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "026ed6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:199 training batch:5257 total train loss:0.214 validation_mae:25.538 validation R2: 0.38 validation MAPE: 1.607:   0%| | 0/5\n"
     ]
    }
   ],
   "source": [
    "nodes_dist = pd.read_csv('nodes_dist.csv')\n",
    "G = nx.read_gpickle('graph.pickle')\n",
    "od = pd.read_csv('inoutwide.csv')\n",
    "pbar = tqdm(list(G.nodes))\n",
    "count = 0\n",
    "\n",
    "for station in pbar:\n",
    "    \n",
    "#     neighbors = list(nx.ego_graph(G, radius=1, n=station, distance='weight', center=False).nodes)\n",
    "    neighbors = nodes_dist.loc[(nodes_dist['o']==station)&\\\n",
    "                               (nodes_dist['dist']<1)&\\\n",
    "                               (nodes_dist['o']!=nodes_dist['d'])]['d'].values.tolist()\n",
    "    flows = ['incoming_flow-'+station,'outgoing_flow-'+station]+\\\n",
    "            ['incoming_flow-'+i for i in neighbors]+\\\n",
    "                      ['outgoing_flow-'+i for i in neighbors]\n",
    "\n",
    "    subod = od[['Date','Hour']+flows]\n",
    "    lags = list(range(1,24))+list(np.array(list(range(2,8)))*24)+list(np.array(list(range(2,8)))*24*7)\n",
    "    for lag in lags:\n",
    "        temp = subod[flows].shift(lag)\n",
    "        temp.columns = [i+'-lag-'+str(lag) for i in flows]\n",
    "        subod = pd.concat([subod,temp],axis=1)\n",
    "    subod = subod.dropna()\n",
    "#     print('here')\n",
    "    subod_melt = subod.drop(columns=flows).melt(id_vars=['Date','Hour'])\n",
    "    subod_melt = subod_melt.sort_values(by=['Date','Hour'])\n",
    "    subod_melt['nearby'] = subod_melt['variable'].apply(lambda x:x.split('flow-')[1].split('-')[0])\n",
    "    subod_melt['degree'] = subod_melt.apply(lambda x:nx.shortest_path_length(G,station,x['nearby']),axis=1)\n",
    "    subod_melt['path_distance'] = subod_melt.apply(lambda x:nx.dijkstra_path_length(G,station,x['nearby']),axis=1)\n",
    "    subod_melt['lag'] = subod_melt['variable'].apply(lambda x: int(x.split('-')[-1]))\n",
    "    subod_melt['variable_test'] = subod_melt['variable'].apply(lambda x: x.split('-lag')[0] if '-lag' in x else x)\n",
    "    subod_melt.sort_values(by=['Date','Hour','variable_test','lag'])\n",
    "    fts = subod_melt[['value','degree','path_distance']]\n",
    "    # len(lags) is number of lags, 2 is bidirectional\n",
    "    fts_train = fts.iloc[:-24*7*2*len(lags)*(len(neighbors)+1),:].values\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(fts_train)\n",
    "    fts_train = torch.tensor(scaler.transform(fts_train))\n",
    "\n",
    "    y = subod[['incoming_flow-'+station,'outgoing_flow-'+station]]\n",
    "    y_train = y.iloc[:-24*7,:].values\n",
    "    y_scaler = StandardScaler()\n",
    "    y_scaler.fit(y_train)\n",
    "    y_train = y_scaler.transform(y_train)\n",
    "    y_train = np.repeat(y_train,len(lags)*2*(len(neighbors)+1),0)\n",
    "    y_train = torch.tensor(y_train)\n",
    "    train_dataset = torch.utils.data.TensorDataset(fts_train,y_train)\n",
    " \n",
    "\n",
    "    fts_val = torch.tensor(scaler.transform(fts.iloc[-24*7*2*len(lags)*(len(neighbors)+1):,:].values)) \n",
    "    y_val = y.iloc[-24*7:,:].values\n",
    "    y_val = np.repeat(y_val,len(lags)*2*(len(neighbors)+1),0)\n",
    "    y_val = torch.tensor(y_val)\n",
    "\n",
    "                      \n",
    "    validation_dataset = torch.utils.data.TensorDataset(fts_val, y_val)\n",
    "    \n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "    N_EPOCHS = 200\n",
    "    BATCH_SIZE = int(1*(len(neighbors)+1)*len(lags))*2\n",
    "#     print('here')\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                                                   num_workers=0)\n",
    "    validation_dataloader = torch.utils.data.DataLoader(validation_dataset,\n",
    "                                                        batch_size=BATCH_SIZE,\n",
    "                                                        num_workers=0)\n",
    "    model = relevance_VNN(input_size=fts.shape[1],n_feature=64,output_size=2)\n",
    "    model = model.to(device)\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01) \n",
    "#     qbar = tqdm(range(N_EPOCHS), leave=False)\n",
    "    validation_mae,validation_r2,validation_mape = 999,999,999\n",
    "    for i in range(N_EPOCHS): \n",
    "\n",
    "        total_train_loss = 0.0\n",
    "        model.train()\n",
    "        training_batch_count = 1\n",
    "        for batch in train_dataloader:\n",
    "\n",
    "#             print('here')\n",
    "            y_train,y_true,loss = get_loss_and_metrics(model, batch, criterion, device)\n",
    "            step(loss.float(), optimizer)\n",
    "            total_train_loss += loss.item()\n",
    "            mean_train_loss = total_train_loss / training_batch_count\n",
    "            pbar.set_description('epoch:' + str(i) +\\\n",
    "                ' training batch:' + str(training_batch_count) +\\\n",
    "                ' total train loss:'+ str(round(mean_train_loss,3))+ \\\n",
    "                 ' validation_mae:'+ str(round(validation_mae,3)) +\\\n",
    "                ' validation R2: '+ str(round(validation_r2,3)) +\\\n",
    "                ' validation MAPE: '+ str(round(validation_mape,3)))\n",
    "            training_batch_count += 1\n",
    "\n",
    "        batch_count = 0\n",
    "        \n",
    "#         if i%50 == 0 and i >=50:\n",
    "        if i >= 0:\n",
    "            for batch in validation_dataloader:\n",
    "    #             print(batch_count)\n",
    "                with torch.no_grad(): \n",
    "                    y_pred,y_true,loss = get_loss_and_metrics(model, batch, criterion, device)\n",
    "                    y_pred,y_true = y_pred.cpu().numpy(),y_true.cpu().numpy()\n",
    "                    y_pred = y_scaler.inverse_transform(y_pred.reshape(1, 2))\n",
    "                    y_true = y_true.reshape(1,2)\n",
    "\n",
    "                    if batch_count == 0: \n",
    "                        y_true_batch = y_true\n",
    "                        y_pred_batch = y_pred\n",
    "                    else:\n",
    "                        y_true_batch = np.concatenate([y_true_batch,y_true])\n",
    "                        y_pred_batch = np.concatenate([y_pred_batch,y_pred])\n",
    "\n",
    "                        validation_mae = mean_absolute_error(y_true_batch,y_pred_batch)\n",
    "                        validation_r2 = r2_score(y_true_batch,y_pred_batch)\n",
    "                        validation_mape = masked_MAPE(y_true_batch,y_pred_batch)\n",
    "                        pbar.set_description('epoch:' + str(i) +\\\n",
    "                        ' training batch:' + str(training_batch_count) +\\\n",
    "                        ' total train loss:'+ str(round(mean_train_loss,3))+ \\\n",
    "                         ' validation_mae:'+ str(round(validation_mae,3)) +\\\n",
    "                        ' validation R2: '+ str(round(validation_r2,3)) +\\\n",
    "                        ' validation MAPE: '+ str(round(validation_mape,3)))\n",
    "                    batch_count += 1\n",
    "                        \n",
    "                \n",
    "                \n",
    "    if count == 0: \n",
    "        y_true_total = y_true_batch\n",
    "        y_pred_total = y_pred_batch\n",
    "    else:\n",
    "        y_true_total = np.concatenate([y_true_total,y_true_batch])\n",
    "        y_pred_total = np.concatenate([y_pred_total,y_pred_batch])\n",
    "    count += 1\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "966e98aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3800343019404123\n",
      "25.537624\n",
      "42.151985\n",
      "1.6065904029925389\n"
     ]
    }
   ],
   "source": [
    "# y_true,y_pred = y_true.cpu().numpy(),y_pred.cpu().numpy()\n",
    "print(r2_score(y_true_total,y_pred_total))\n",
    "print(mean_absolute_error(y_true_total,y_pred_total))\n",
    "print(mean_squared_error(y_true_total,y_pred_total,squared=False))\n",
    "print(masked_MAPE(y_true_total,y_pred_total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a276f2e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
