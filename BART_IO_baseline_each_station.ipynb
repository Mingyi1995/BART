{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc05a381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import * \n",
    "from sklearn.metrics import r2_score,mean_absolute_error,mean_absolute_percentage_error,mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "import geopandas as gpd\n",
    "# from geopy.distance import distance,geodesic\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import acf, pacf,adfuller, kpss,range_unit_root_test\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c55da5c",
   "metadata": {},
   "source": [
    "# different methods, all tested on 09-26 to 10-02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0988005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_MAPE(v, v_, axis=None):\n",
    "    '''\n",
    "    Mean absolute percentage error.\n",
    "    :param v: np.ndarray or int, ground truth.\n",
    "    :param v_: np.ndarray or int, prediction.\n",
    "    :param axis: axis to do calculation.\n",
    "    :return: int, MAPE averages on all elements of input.\n",
    "    '''\n",
    "    mask = (v == 0)\n",
    "    percentage = np.abs(v_ - v) / np.abs(v)\n",
    "    if np.any(mask):\n",
    "        masked_array = np.ma.masked_array(percentage, mask=mask)  # mask the dividing-zero as invalid\n",
    "        result = masked_array.mean(axis=axis)\n",
    "        if isinstance(result, np.ma.MaskedArray):\n",
    "            return result.filled(np.nan)\n",
    "        else:\n",
    "            return result\n",
    "    return np.mean(percentage, axis).astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e661c6",
   "metadata": {},
   "source": [
    "# station-level incoming/outgoing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb9dd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|███████████████████████████████████████▌                                                    | 43/100 [00:12<00:16,  3.35it/s]"
     ]
    }
   ],
   "source": [
    "od = pd.read_csv('inoutwide.csv')\n",
    "od = od.sort_values(by=['Date','Hour'])\n",
    "od_test = od.iloc[-24*7:,:]\n",
    "## lag linear regression, many to one\n",
    "\n",
    "# 48 continuous hourly lag, 3-7 days ago same hour-of-day lag, 7 weeks ago lag\n",
    "for station in tqdm([i for i in od.columns if '_flow' in i]):\n",
    "    subod = od[['Date','Hour',station,\n",
    " 'dow-0','dow-1','dow-2','dow-3','dow-4','dow-5','dow-6',\n",
    " 'hour-0','hour-1','hour-2','hour-3','hour-4','hour-5','hour-6','hour-7','hour-8','hour-9','hour-10',\n",
    " 'hour-11','hour-12','hour-13','hour-14','hour-15','hour-16','hour-17','hour-18','hour-19','hour-20',\n",
    " 'hour-21','hour-22','hour-23']]\n",
    "    for lag in list(range(1,48))+list(np.array(list(range(3,8)))*24)+list(np.array(list(range(2,8)))*24*7):\n",
    "        temp = subod[[station]].shift(lag)\n",
    "        temp.columns = ['station'+'-lag-'+str(lag)]\n",
    "        subod = pd.concat([subod,temp],axis=1)\n",
    "\n",
    "    subod = subod.sort_values(by=['Date','Hour'])\n",
    "    subod = subod.dropna()\n",
    "\n",
    "    x = subod[[col for col in subod.columns if '-lag-' in col]]\n",
    "    y = subod[[station]]\n",
    "\n",
    "\n",
    "    x_train = x.iloc[:-24*7,:].values\n",
    "    y_train = y.iloc[:-24*7,].values\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "\n",
    "    y_scaler = StandardScaler()\n",
    "    y_scaler.fit(y_train)\n",
    "    y_train = y_scaler.transform(y_train)\n",
    "\n",
    "    model = LinearRegression(fit_intercept=False).fit(x_train, y_train)\n",
    "\n",
    "#     print('out of sample R2')\n",
    "    x_test = x.iloc[-24*7:,:].values\n",
    "    y_test = y.iloc[-24*7:].values\n",
    "\n",
    "    x_test = scaler.transform(x_test)\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred = y_scaler.inverse_transform(y_pred)\n",
    "\n",
    "    od_test[station+'_pred'] = y_pred\n",
    "\n",
    "flow_columns = [i for i in od.columns if '_flow' in i]\n",
    "y_test = od_test[flow_columns].values\n",
    "y_pred = od_test[[i+'_pred' for i in flow_columns]].values\n",
    "print(r2_score(y_test,y_pred))\n",
    "print(mean_absolute_error(y_test,y_pred))\n",
    "print(mean_squared_error(y_test,y_pred,squared=False))\n",
    "print(np.mean(mean_squared_error(y_test,y_pred,multioutput='raw_values',squared=False)/np.std(y_test)))\n",
    "print(masked_MAPE(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c17c91",
   "metadata": {},
   "source": [
    "## adding  surrounding flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a14c3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "od = pd.read_csv('inoutlongNear.csv')\n",
    "\n",
    "od = od.sort_values(by=['station','Date','Hour'])\n",
    "for lag in list(range(1,48))+list(np.array(list(range(3,8)))*24)+list(np.array(list(range(2,8)))*24*7):\n",
    "    temp = od[['station','incoming_flow','outgoing_flow','nearby_incoming','nearby_outgoing']].shift(lag)\n",
    "    temp.columns = ['station'+'-'+str(lag),\n",
    "                    'incoming_flow'+'-'+str(lag),'outgoing_flow'+'-'+str(lag),\n",
    "                    'nearby_incoming'+'-'+str(lag),'nearby_outgoing'+'-'+str(lag)]\n",
    "    od = pd.concat([od,temp],axis=1)\n",
    "    \n",
    "od = od.sort_values(by=['Date','Hour','station'])\n",
    "od = od.dropna()\n",
    "od = od.loc[od['station']==od['station'+'-'+str(lag)]]\n",
    "od = od.drop(columns=[i for i in od.columns if 'station-' in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220e8243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total nearby incoming outgoing\n",
    "pbar = tqdm(od.station.unique())\n",
    "for station in pbar:\n",
    "    i = 0\n",
    "    subod = od.loc[od['station']==station]\n",
    "    x = subod.drop(columns=['Date','Hour','incoming_flow','outgoing_flow','station','nearby_incoming','nearby_outgoing'])\n",
    "    y = subod[['incoming_flow','outgoing_flow']]\n",
    "\n",
    "    # incoming model  training\n",
    "    x_train = x.iloc[:-24*7,:].values\n",
    "    x_test = x.iloc[-24*7:,:].values\n",
    "    y_train_in = y.iloc[:-24*7,0].values.reshape(-1,1)\n",
    "    y_train_out = y.iloc[:-24*7,1].values.reshape(-1,1)\n",
    "    y_test_in = y.iloc[:-24*7,0].values.reshape(-1,1)\n",
    "    y_test_out = y.iloc[-24*7:,1].values.reshape(-1,1)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "    \n",
    "    y_in_scaler = StandardScaler()\n",
    "    y_in_scaler.fit(y_train_in)\n",
    "    y_train_in = y_in_scaler.transform(y_train_in)\n",
    "    \n",
    "    model_in = LinearRegression(fit_intercept=False).fit(x_train, y_train_in)\n",
    "    y_pred_in = model_in.predict(x_test)\n",
    "    y_pred_in = y_in_scaler.inverse_transform(y_pred_in)\n",
    "    \n",
    "    y_pred_in_train = model_in.predict(x_train)\n",
    "    y_pred_in_train = y_in_scaler.inverse_transform(y_pred_in_train)\n",
    "    \n",
    "    in_trian_r2_score = r2_score(y_in_scaler.inverse_transform(y_train_in),y_pred_in_train)\n",
    "    in_test_r2_score = r2_score(y_test_in,y_pred_in_train)\n",
    "    # outgoing model training\n",
    "    \n",
    "    y_out_scaler = StandardScaler()\n",
    "    y_out_scaler.fit(y_train_out)\n",
    "    y_train_out = y_out_scaler.transform(y_train_out)\n",
    "\n",
    "    model_out = LinearRegression(fit_intercept=False).fit(x_train, y_train_out)\n",
    "    y_pred_out = model_out.predict(x_test)\n",
    "    y_pred_out = y_out_scaler.inverse_transform(y_pred_out)\n",
    "    \n",
    "    y_pred_out_train = model_out.predict(x_train)\n",
    "    y_pred_out_train = y_in_scaler.inverse_transform(y_pred_out_train)\n",
    "    \n",
    "    out_trian_r2_score = r2_score(y_in_scaler.inverse_transform(y_train_out),y_pred_out_train)\n",
    "    out_test_r2_score = r2_score(y_test_out,y_pred_out)\n",
    "    if i == 0: \n",
    "        y_true = y.iloc[-24*7:,:].values\n",
    "        y_pred = np.concatenate([y_pred_in,y_pred_out],axis=1)\n",
    "    else:\n",
    "        y_true = np.concatenate([y_true,y.iloc[-24*7:,:].values])\n",
    "        y_pred = np.concatenate([y_true,np.concatenate([y_pred_in,y_pred_out],axis=1)])\n",
    "    i += 1\n",
    "    \n",
    "    pbar.set_description('incoming_train_R2:'+ str(round(in_trian_r2_score,4))+\\\n",
    "                        'incoming_test_R2:'+ str(round(in_test_r2_score,4))+\\\n",
    "                         'outgoing_train_R2:'+ str(round(out_trian_r2_score,4))+\\\n",
    "                        'outgoing_test_R2:'+ str(round(out_test_r2_score,4)))\n",
    "    \n",
    "print(r2_score(y_true,y_pred))\n",
    "print(mean_absolute_error(y_true,y_pred))\n",
    "print(mean_squared_error(y_true,y_pred,squared=False))\n",
    "print(np.mean(mean_squared_error(y_true,y_pred,multioutput='raw_values',squared=False)/np.std(y_true)))\n",
    "print(masked_MAPE(y_true,y_pred))    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d738b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nearby stations incoming outgoing, separately\n",
    "od = pd.read_csv('inoutwide.csv')\n",
    "G = nx.read_gpickle('graph.pickle')\n",
    "pbar = tqdm(list(G.nodes))\n",
    "for station in pbar:\n",
    "    i = 0\n",
    "    neightbor = list(nx.ego_graph(G, radius=1, n=station, distance='weight', center=False).nodes)\n",
    "    flows = ['incoming_flow-'+i for i in [station]+neightbor]+\\\n",
    "              ['outgoing_flow-'+i for i in [station]+neightbor]\n",
    "    subod = od[['Date','Hour']+['hour-'+str(i) for i in range(24)]+['dow-'+str(i) for i in range(7)]+flows]\n",
    "    \n",
    "    for lag in list(range(1,48))+list(np.array(list(range(3,8)))*24)+list(np.array(list(range(2,8)))*24*7):\n",
    "        temp = subod[flows].shift(lag)\n",
    "        temp.columns = [i+'-lag-'+str(lag) for i in flows]\n",
    "        subod = pd.concat([subod,temp],axis=1)\n",
    "\n",
    "    subod = subod.sort_values(by=['Date','Hour'])\n",
    "    subod = subod.dropna()\n",
    "    lags = [i for i in subod.columns if 'lag' in i]\n",
    "    x = subod[lags+['hour-'+str(i) for i in range(24)]+['dow-'+str(i) for i in range(7)]]\n",
    "    y = subod[['incoming_flow-'+station,'outgoing_flow-'+station]]\n",
    "\n",
    "    # incoming model  training\n",
    "    x_train = x.iloc[:-24*7,:].values\n",
    "    x_test = x.iloc[-24*7:,:].values\n",
    "    y_train_in = y.iloc[:-24*7,0].values.reshape(-1,1)\n",
    "    y_train_out = y.iloc[:-24*7,1].values.reshape(-1,1)\n",
    "    y_test_in = y.iloc[:-24*7,0].values.reshape(-1,1)\n",
    "    y_test_out = y.iloc[-24*7:,1].values.reshape(-1,1)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "    \n",
    "    y_in_scaler = StandardScaler()\n",
    "    y_in_scaler.fit(y_train_in)\n",
    "    y_train_in = y_in_scaler.transform(y_train_in)\n",
    "    \n",
    "    model_in = LinearRegression(fit_intercept=False).fit(x_train, y_train_in)\n",
    "    y_pred_in = model_in.predict(x_test)\n",
    "    y_pred_in = y_in_scaler.inverse_transform(y_pred_in)\n",
    "    \n",
    "    y_pred_in_train = model_in.predict(x_train)\n",
    "    y_pred_in_train = y_in_scaler.inverse_transform(y_pred_in_train)\n",
    "    \n",
    "    in_trian_r2_score = r2_score(y_in_scaler.inverse_transform(y_train_in),y_pred_in_train)\n",
    "    in_test_r2_score = r2_score(y_test_in,y_pred_in_train)\n",
    "    # outgoing model training\n",
    "    \n",
    "    y_out_scaler = StandardScaler()\n",
    "    y_out_scaler.fit(y_train_out)\n",
    "    y_train_out = y_out_scaler.transform(y_train_out)\n",
    "\n",
    "    model_out = LinearRegression(fit_intercept=False).fit(x_train, y_train_out)\n",
    "    y_pred_out = model_out.predict(x_test)\n",
    "    y_pred_out = y_out_scaler.inverse_transform(y_pred_out)\n",
    "    \n",
    "    y_pred_out_train = model_out.predict(x_train)\n",
    "    y_pred_out_train = y_in_scaler.inverse_transform(y_pred_out_train)\n",
    "    \n",
    "    out_trian_r2_score = r2_score(y_in_scaler.inverse_transform(y_train_out),y_pred_out_train)\n",
    "    out_test_r2_score = r2_score(y_test_out,y_pred_out)\n",
    "    if i == 0: \n",
    "        y_true = y.iloc[-24*7:,:].values\n",
    "        y_pred = np.concatenate([y_pred_in,y_pred_out],axis=1)\n",
    "    else:\n",
    "        y_true = np.concatenate([y_true,y.iloc[-24*7:,:].values])\n",
    "        y_pred = np.concatenate([y_true,np.concatenate([y_pred_in,y_pred_out],axis=1)])\n",
    "    i += 1\n",
    "    \n",
    "    pbar.set_description('incoming_train_R2:'+ str(round(in_trian_r2_score,4))+\\\n",
    "                        'incoming_test_R2:'+ str(round(in_test_r2_score,4))+\\\n",
    "                         'outgoing_train_R2:'+ str(round(out_trian_r2_score,4))+\\\n",
    "                        'outgoing_test_R2:'+ str(round(out_test_r2_score,4)))\n",
    "    \n",
    "print(r2_score(y_true,y_pred))\n",
    "print(mean_absolute_error(y_true,y_pred))\n",
    "print(mean_squared_error(y_true,y_pred,squared=False))\n",
    "print(np.mean(mean_squared_error(y_true,y_pred,multioutput='raw_values',squared=False)/np.std(y_true)))\n",
    "print(masked_MAPE(y_true,y_pred))    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b6c68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b72e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VNN(nn.Module):\n",
    "    def __init__(self,input_size,n_feature=128):\n",
    "        super(VNN, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.weight = nn.Sequential(\n",
    "        nn.Linear(input_size, n_feature),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Linear(n_feature,n_feature),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Linear(n_feature,n_feature),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Linear(n_feature,2),\n",
    "        )\n",
    " \n",
    "    def forward(self,x):\n",
    "        flow = self.weight(x)\n",
    "        return flow\n",
    "\n",
    "\n",
    "def get_loss_and_metrics(model, data, target,criterion, device):\n",
    "  # Implement forward pass and loss calculation for one batch.\n",
    "  # Remember to move the batch to device.\n",
    "  # \n",
    "  # Return a tuple:\n",
    "  # - loss for the batch (Tensor)\n",
    "  # - number of correctly classified examples in the batch (Tensor)\n",
    "#     data, target = batch[0], batch[1]\n",
    "#     print(data.shape)\n",
    "    data = torch.tensor(data, dtype=torch.float32)\n",
    "    target = torch.tensor(target, dtype=torch.float32)\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(data)\n",
    "#     print(pred,target)\n",
    "    loss = criterion(pred, target)\n",
    "    \n",
    "    \n",
    "    return (pred,target,loss)\n",
    "    \n",
    "def step(loss, optimizer):\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026ed6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_dist = pd.read_csv('nodes_dist.csv')\n",
    "G = nx.read_gpickle('graph.pickle')\n",
    "od = pd.read_csv('inoutwide.csv')\n",
    "pbar = tqdm(list(G.nodes))\n",
    "count = 0\n",
    "\n",
    "for station in pbar:\n",
    "    \n",
    "#     neighbors = list(nx.ego_graph(G, radius=1, n=station, distance='weight', center=False).nodes)\n",
    "    neighbors = nodes_dist.loc[(nodes_dist['o']==station)&\\\n",
    "                               (nodes_dist['dist']<1)&\\\n",
    "                               (nodes_dist['o']!=nodes_dist['d'])]['d'].values.tolist()\n",
    "    flows = ['incoming_flow-'+station]+\\\n",
    "            ['incoming_flow-'+i for i in neighbors]+['outgoing_flow-'+station]+\\\n",
    "                      ['outgoing_flow-'+i for i in neighbors]\n",
    "\n",
    "    fts = od[['Date','Hour']+flows]\n",
    "    lags = list(range(1,24))+list(np.array(list(range(2,8)))*24)+list(np.array(list(range(2,8)))*24*7)\n",
    "    for lag in lags:\n",
    "        temp = fts[flows].shift(lag)\n",
    "        temp.columns = [i+'-lag-'+str(lag) for i in flows]\n",
    "        fts = pd.concat([fts,temp],axis=1)\n",
    "    fts = fts.dropna()\n",
    "    fts = fts.drop(columns=['Date','Hour'])\n",
    "\n",
    "    \n",
    "    # len(lags) is number of lags, 2 is bidirectional\n",
    "    # remove flows at time t\n",
    "    fts_train = fts.iloc[:-24*7,(len(neighbors)+1)*2:].values\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(fts_train)\n",
    "    fts_train = torch.tensor(scaler.transform(fts_train))\n",
    "\n",
    "    y = fts[['incoming_flow-'+station,'outgoing_flow-'+station]]\n",
    "    y_train = y.iloc[:-24*7,:].values\n",
    "    y_scaler = StandardScaler()\n",
    "    y_scaler.fit(y_train)\n",
    "    y_train = y_scaler.transform(y_train)\n",
    "    y_train = torch.tensor(y_train)\n",
    "    train_dataset = torch.utils.data.TensorDataset(fts_train,y_train)\n",
    " \n",
    "\n",
    "    fts_val = torch.tensor(scaler.transform(fts.iloc[-24*7:,(len(neighbors)+1)*2:].values)) \n",
    "    y_val = y.iloc[-24*7:,:].values\n",
    "    y_val = torch.tensor(y_val)    \n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "    N_EPOCHS = 501\n",
    "    BATCH_SIZE = 64\n",
    "#     print('here')\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                                                   num_workers=0)\n",
    "\n",
    "    model = VNN(input_size=(1+len(neighbors))*len(lags)*2)\n",
    "    model = model.to(device)\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01) \n",
    "#     qbar = tqdm(range(N_EPOCHS), leave=False)\n",
    "    validation_mae,validation_r2,validation_mape = 999,999,999\n",
    "    for i in range(N_EPOCHS): \n",
    "\n",
    "        total_train_loss = 0.0\n",
    "        model.train()\n",
    "        training_batch_count = 1\n",
    "        for batch in train_dataloader:\n",
    "            y_train,y_true,loss = get_loss_and_metrics(model,batch[0],batch[1], criterion, device)\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            mean_train_loss = total_train_loss / training_batch_count\n",
    "            pbar.set_description('epoch:' + str(i) +\\\n",
    "                ' training batch:' + str(training_batch_count) +\\\n",
    "                ' total train loss:'+ str(round(mean_train_loss,3))+ \\\n",
    "                 ' validation_mae:'+ str(round(validation_mae,3)) +\\\n",
    "                ' validation R2: '+ str(round(validation_r2,3)) +\\\n",
    "                ' validation MAPE: '+ str(round(validation_mape,3)))\n",
    "            training_batch_count += 1\n",
    "            \n",
    "            step(loss,optimizer)\n",
    "        \n",
    "        if i%50 == 0 and i >=50:\n",
    "#         if i >= 0:\n",
    "            with torch.no_grad(): \n",
    "                \n",
    "                y_pred,y_true,loss = get_loss_and_metrics(model, fts_val, y_val, criterion, device)\n",
    "                y_pred_val,y_true_val = y_pred.cpu().numpy(),y_true.cpu().numpy()\n",
    "                y_pred_val = y_scaler.inverse_transform(y_pred_val)\n",
    "\n",
    "                validation_mae = mean_absolute_error(y_true_val,y_pred_val)\n",
    "                validation_r2 = r2_score(y_true_val,y_pred_val)\n",
    "                validation_mape = masked_MAPE(y_true_val,y_pred_val)\n",
    "                pbar.set_description('epoch:' + str(i) +\\\n",
    "                ' training batch:' + str(training_batch_count) +\\\n",
    "                ' total train loss:'+ str(round(mean_train_loss,3))+ \\\n",
    "                 ' validation_mae:'+ str(round(validation_mae,3)) +\\\n",
    "                ' validation R2: '+ str(round(validation_r2,3)) +\\\n",
    "                ' validation MAPE: '+ str(round(validation_mape,3)))\n",
    "                        \n",
    "                \n",
    "                \n",
    "    if count == 0: \n",
    "        y_true_total = y_true_val\n",
    "        y_pred_total = y_pred_val\n",
    "    else:\n",
    "        y_true_total = np.concatenate([y_true_total,y_true_val])\n",
    "        y_pred_total = np.concatenate([y_pred_total,y_pred_val])\n",
    "    count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6f28ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9732133272481034\n",
      "17.420902\n",
      "36.256836\n",
      "0.16361974\n",
      "0.4888177925810495\n"
     ]
    }
   ],
   "source": [
    "# y_true,y_pred = y_true.cpu().numpy(),y_pred.cpu().numpy()\n",
    "print(r2_score(y_true_total,y_pred_total))\n",
    "print(mean_absolute_error(y_true_total,y_pred_total))\n",
    "print(mean_squared_error(y_true_total,y_pred_total,squared=False))\n",
    "print(mean_squared_error(y_true_total,y_pred_total,squared=False)/np.mean(np.std(y_true_total)))\n",
    "\n",
    "print(masked_MAPE(y_true_total,y_pred_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f5233c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dc6f96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
