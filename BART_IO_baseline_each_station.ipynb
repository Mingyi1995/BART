{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc05a381",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/gnn/lib/python3.10/site-packages/geopandas/_compat.py:123: UserWarning: The Shapely GEOS version (3.10.3-CAPI-1.16.1) is incompatible with the GEOS version PyGEOS was compiled with (3.11.0-CAPI-1.17.0). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import * \n",
    "from sklearn.metrics import r2_score,mean_absolute_error,mean_absolute_percentage_error,mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "import geopandas as gpd\n",
    "# from geopy.distance import distance,geodesic\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import acf, pacf,adfuller, kpss,range_unit_root_test\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c55da5c",
   "metadata": {},
   "source": [
    "# different methods, all tested on 09-26 to 10-02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0988005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_MAPE(v, v_, axis=None):\n",
    "    '''\n",
    "    Mean absolute percentage error.\n",
    "    :param v: np.ndarray or int, ground truth.\n",
    "    :param v_: np.ndarray or int, prediction.\n",
    "    :param axis: axis to do calculation.\n",
    "    :return: int, MAPE averages on all elements of input.\n",
    "    '''\n",
    "    mask = (v == 0)\n",
    "    percentage = np.abs(v_ - v) / np.abs(v)\n",
    "    if np.any(mask):\n",
    "        masked_array = np.ma.masked_array(percentage, mask=mask)  # mask the dividing-zero as invalid\n",
    "        result = masked_array.mean(axis=axis)\n",
    "        if isinstance(result, np.ma.MaskedArray):\n",
    "            return result.filled(np.nan)\n",
    "        else:\n",
    "            return result\n",
    "    return np.mean(percentage, axis).astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e661c6",
   "metadata": {},
   "source": [
    "# station-level incoming/outgoing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecb9dd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 100/100 [02:24<00:00,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9387008610819879\n",
      "16.6033665998665\n",
      "26.67203137438082\n",
      "0.12036546191120305\n",
      "0.41237860129069803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "od = pd.read_csv('inoutwide.csv')\n",
    "od = od.sort_values(by=['Date','Hour'])\n",
    "od_test = od.iloc[-24*7:,:]\n",
    "## lag linear regression, many to one\n",
    "\n",
    "# 48 continuous hourly lag, 3-7 days ago same hour-of-day lag, 7 weeks ago lag\n",
    "for station in tqdm([i for i in od.columns if '_flow' in i]):\n",
    "    subod = od[['Date','Hour',station,\n",
    " 'dow-0','dow-1','dow-2','dow-3','dow-4','dow-5','dow-6',\n",
    " 'hour-0','hour-1','hour-2','hour-3','hour-4','hour-5','hour-6','hour-7','hour-8','hour-9','hour-10',\n",
    " 'hour-11','hour-12','hour-13','hour-14','hour-15','hour-16','hour-17','hour-18','hour-19','hour-20',\n",
    " 'hour-21','hour-22','hour-23']]\n",
    "    for lag in list(range(1,48))+list(np.array(list(range(3,8)))*24)+list(np.array(list(range(2,8)))*24*7):\n",
    "        temp = subod[[station]].shift(lag)\n",
    "        temp.columns = ['station'+'-lag-'+str(lag)]\n",
    "        subod = pd.concat([subod,temp],axis=1)\n",
    "\n",
    "    subod = subod.sort_values(by=['Date','Hour'])\n",
    "    subod = subod.dropna()\n",
    "\n",
    "    x = subod[[col for col in subod.columns if '-lag-' in col]]\n",
    "    y = subod[[station]]\n",
    "\n",
    "\n",
    "    x_train = x.iloc[:-24*7,:].values\n",
    "    y_train = y.iloc[:-24*7,].values\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "\n",
    "    y_scaler = StandardScaler()\n",
    "    y_scaler.fit(y_train)\n",
    "    y_train = y_scaler.transform(y_train)\n",
    "\n",
    "    model = LinearRegression(fit_intercept=False).fit(x_train, y_train)\n",
    "\n",
    "#     print('out of sample R2')\n",
    "    x_test = x.iloc[-24*7:,:].values\n",
    "    y_test = y.iloc[-24*7:].values\n",
    "\n",
    "    x_test = scaler.transform(x_test)\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred = y_scaler.inverse_transform(y_pred)\n",
    "\n",
    "    od_test[station+'_pred'] = y_pred\n",
    "\n",
    "flow_columns = [i for i in od.columns if '_flow' in i]\n",
    "y_test = od_test[flow_columns].values\n",
    "y_pred = od_test[[i+'_pred' for i in flow_columns]].values\n",
    "print(r2_score(y_test,y_pred))\n",
    "print(mean_absolute_error(y_test,y_pred))\n",
    "print(mean_squared_error(y_test,y_pred,squared=False))\n",
    "print(np.mean(mean_squared_error(y_test,y_pred,multioutput='raw_values',squared=False)/np.std(y_test)))\n",
    "print(masked_MAPE(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c17c91",
   "metadata": {},
   "source": [
    "## adding  surrounding flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a14c3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "od = pd.read_csv('inoutlongNear.csv')\n",
    "\n",
    "od = od.sort_values(by=['station','Date','Hour'])\n",
    "for lag in list(range(1,48))+list(np.array(list(range(3,8)))*24)+list(np.array(list(range(2,8)))*24*7):\n",
    "    temp = od[['station','incoming_flow','outgoing_flow','nearby_incoming','nearby_outgoing']].shift(lag)\n",
    "    temp.columns = ['station'+'-'+str(lag),\n",
    "                    'incoming_flow'+'-'+str(lag),'outgoing_flow'+'-'+str(lag),\n",
    "                    'nearby_incoming'+'-'+str(lag),'nearby_outgoing'+'-'+str(lag)]\n",
    "    od = pd.concat([od,temp],axis=1)\n",
    "    \n",
    "od = od.sort_values(by=['Date','Hour','station'])\n",
    "od = od.dropna()\n",
    "od = od.loc[od['station']==od['station'+'-'+str(lag)]]\n",
    "od = od.drop(columns=[i for i in od.columns if 'station-' in i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "220e8243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9662666138880707\n",
      "16.912960001963743\n",
      "24.40287604161488\n",
      "0.11012522464007464\n",
      "0.42478471631772374\n"
     ]
    }
   ],
   "source": [
    "# total nearby incoming outgoing\n",
    "pbar = tqdm(od.station.unique())\n",
    "for station in pbar:\n",
    "    i = 0\n",
    "    subod = od.loc[od['station']==station]\n",
    "    x = subod.drop(columns=['Date','Hour','incoming_flow','outgoing_flow','station','nearby_incoming','nearby_outgoing'])\n",
    "    y = subod[['incoming_flow','outgoing_flow']]\n",
    "\n",
    "    # incoming model  training\n",
    "    x_train = x.iloc[:-24*7,:].values\n",
    "    x_test = x.iloc[-24*7:,:].values\n",
    "    y_train_in = y.iloc[:-24*7,0].values.reshape(-1,1)\n",
    "    y_train_out = y.iloc[:-24*7,1].values.reshape(-1,1)\n",
    "    y_test_in = y.iloc[:-24*7,0].values.reshape(-1,1)\n",
    "    y_test_out = y.iloc[-24*7:,1].values.reshape(-1,1)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "    \n",
    "    y_in_scaler = StandardScaler()\n",
    "    y_in_scaler.fit(y_train_in)\n",
    "    y_train_in = y_in_scaler.transform(y_train_in)\n",
    "    \n",
    "    model_in = LinearRegression(fit_intercept=False).fit(x_train, y_train_in)\n",
    "    y_pred_in = model_in.predict(x_test)\n",
    "    y_pred_in = y_in_scaler.inverse_transform(y_pred_in)\n",
    "    \n",
    "    y_pred_in_train = model_in.predict(x_train)\n",
    "    y_pred_in_train = y_in_scaler.inverse_transform(y_pred_in_train)\n",
    "    \n",
    "    in_trian_r2_score = r2_score(y_in_scaler.inverse_transform(y_train_in),y_pred_in_train)\n",
    "    in_test_r2_score = r2_score(y_test_in,y_pred_in_train)\n",
    "    # outgoing model training\n",
    "    \n",
    "    y_out_scaler = StandardScaler()\n",
    "    y_out_scaler.fit(y_train_out)\n",
    "    y_train_out = y_out_scaler.transform(y_train_out)\n",
    "\n",
    "    model_out = LinearRegression(fit_intercept=False).fit(x_train, y_train_out)\n",
    "    y_pred_out = model_out.predict(x_test)\n",
    "    y_pred_out = y_out_scaler.inverse_transform(y_pred_out)\n",
    "    \n",
    "    y_pred_out_train = model_out.predict(x_train)\n",
    "    y_pred_out_train = y_in_scaler.inverse_transform(y_pred_out_train)\n",
    "    \n",
    "    out_trian_r2_score = r2_score(y_in_scaler.inverse_transform(y_train_out),y_pred_out_train)\n",
    "    out_test_r2_score = r2_score(y_test_out,y_pred_out)\n",
    "    if i == 0: \n",
    "        y_true = y.iloc[-24*7:,:].values\n",
    "        y_pred = np.concatenate([y_pred_in,y_pred_out],axis=1)\n",
    "    else:\n",
    "        y_true = np.concatenate([y_true,y.iloc[-24*7:,:].values])\n",
    "        y_pred = np.concatenate([y_true,np.concatenate([y_pred_in,y_pred_out],axis=1)])\n",
    "    i += 1\n",
    "    \n",
    "    pbar.set_description('incoming_train_R2:'+ str(round(in_trian_r2_score,4))+\\\n",
    "                        'incoming_test_R2:'+ str(round(in_test_r2_score,4))+\\\n",
    "                         'outgoing_train_R2:'+ str(round(out_trian_r2_score,4))+\\\n",
    "                        'outgoing_test_R2:'+ str(round(out_test_r2_score,4)))\n",
    "    \n",
    "print(r2_score(y_true,y_pred))\n",
    "print(mean_absolute_error(y_true,y_pred))\n",
    "print(mean_squared_error(y_true,y_pred,squared=False))\n",
    "print(np.mean(mean_squared_error(y_true,y_pred,multioutput='raw_values',squared=False)/np.std(y_test)))\n",
    "print(masked_MAPE(y_true,y_pred))    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3d738b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "incoming_train_R2:0.9529incoming_test_R2:0.9529outgoing_train_R2:0.9535outgoing_test_R2:0.9747: 100%|█| 50/50 [00:17<00:00,  2.85i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9674945899484979\n",
      "10.053871554330986\n",
      "15.665256287706036\n",
      "0.07069412083993876\n",
      "0.3914008420693012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# nearby stations incoming outgoing, separately\n",
    "od = pd.read_csv('inoutwide.csv')\n",
    "G = nx.read_gpickle('graph.pickle')\n",
    "pbar = tqdm(list(G.nodes))\n",
    "for station in pbar:\n",
    "    i = 0\n",
    "    neightbor = list(nx.ego_graph(G, radius=1, n=station, distance='weight', center=False).nodes)\n",
    "    flows = ['incoming_flow-'+i for i in [station]+neightbor]+\\\n",
    "              ['outgoing_flow-'+i for i in [station]+neightbor]\n",
    "    subod = od[['Date','Hour']+['hour-'+str(i) for i in range(24)]+['dow-'+str(i) for i in range(7)]+flows]\n",
    "    \n",
    "    for lag in list(range(1,48))+list(np.array(list(range(3,8)))*24)+list(np.array(list(range(2,8)))*24*7):\n",
    "        temp = subod[flows].shift(lag)\n",
    "        temp.columns = [i+'-lag-'+str(lag) for i in flows]\n",
    "        subod = pd.concat([subod,temp],axis=1)\n",
    "\n",
    "    subod = subod.sort_values(by=['Date','Hour'])\n",
    "    subod = subod.dropna()\n",
    "    lags = [i for i in subod.columns if 'lag' in i]\n",
    "    x = subod[lags+['hour-'+str(i) for i in range(24)]+['dow-'+str(i) for i in range(7)]]\n",
    "    y = subod[['incoming_flow-'+station,'outgoing_flow-'+station]]\n",
    "\n",
    "    # incoming model  training\n",
    "    x_train = x.iloc[:-24*7,:].values\n",
    "    x_test = x.iloc[-24*7:,:].values\n",
    "    y_train_in = y.iloc[:-24*7,0].values.reshape(-1,1)\n",
    "    y_train_out = y.iloc[:-24*7,1].values.reshape(-1,1)\n",
    "    y_test_in = y.iloc[:-24*7,0].values.reshape(-1,1)\n",
    "    y_test_out = y.iloc[-24*7:,1].values.reshape(-1,1)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "    \n",
    "    y_in_scaler = StandardScaler()\n",
    "    y_in_scaler.fit(y_train_in)\n",
    "    y_train_in = y_in_scaler.transform(y_train_in)\n",
    "    \n",
    "    model_in = LinearRegression(fit_intercept=False).fit(x_train, y_train_in)\n",
    "    y_pred_in = model_in.predict(x_test)\n",
    "    y_pred_in = y_in_scaler.inverse_transform(y_pred_in)\n",
    "    \n",
    "    y_pred_in_train = model_in.predict(x_train)\n",
    "    y_pred_in_train = y_in_scaler.inverse_transform(y_pred_in_train)\n",
    "    \n",
    "    in_trian_r2_score = r2_score(y_in_scaler.inverse_transform(y_train_in),y_pred_in_train)\n",
    "    in_test_r2_score = r2_score(y_test_in,y_pred_in_train)\n",
    "    # outgoing model training\n",
    "    \n",
    "    y_out_scaler = StandardScaler()\n",
    "    y_out_scaler.fit(y_train_out)\n",
    "    y_train_out = y_out_scaler.transform(y_train_out)\n",
    "\n",
    "    model_out = LinearRegression(fit_intercept=False).fit(x_train, y_train_out)\n",
    "    y_pred_out = model_out.predict(x_test)\n",
    "    y_pred_out = y_out_scaler.inverse_transform(y_pred_out)\n",
    "    \n",
    "    y_pred_out_train = model_out.predict(x_train)\n",
    "    y_pred_out_train = y_in_scaler.inverse_transform(y_pred_out_train)\n",
    "    \n",
    "    out_trian_r2_score = r2_score(y_in_scaler.inverse_transform(y_train_out),y_pred_out_train)\n",
    "    out_test_r2_score = r2_score(y_test_out,y_pred_out)\n",
    "    if i == 0: \n",
    "        y_true = y.iloc[-24*7:,:].values\n",
    "        y_pred = np.concatenate([y_pred_in,y_pred_out],axis=1)\n",
    "    else:\n",
    "        y_true = np.concatenate([y_true,y.iloc[-24*7:,:].values])\n",
    "        y_pred = np.concatenate([y_true,np.concatenate([y_pred_in,y_pred_out],axis=1)])\n",
    "    i += 1\n",
    "    \n",
    "    pbar.set_description('incoming_train_R2:'+ str(round(in_trian_r2_score,4))+\\\n",
    "                        'incoming_test_R2:'+ str(round(in_test_r2_score,4))+\\\n",
    "                         'outgoing_train_R2:'+ str(round(out_trian_r2_score,4))+\\\n",
    "                        'outgoing_test_R2:'+ str(round(out_test_r2_score,4)))\n",
    "    \n",
    "print(r2_score(y_true,y_pred))\n",
    "print(mean_absolute_error(y_true,y_pred))\n",
    "print(mean_squared_error(y_true,y_pred,squared=False))\n",
    "print(np.mean(mean_squared_error(y_true,y_pred,multioutput='raw_values',squared=False)/np.std(y_test)))\n",
    "print(masked_MAPE(y_true,y_pred))    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80b6c68b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3b72e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class relevance_VNN(nn.Module):\n",
    "    def __init__(self,input_size,n_neighbor, n_feature, output_size):\n",
    "        super(relevance_VNN, self).__init__()\n",
    "        \n",
    "        self.relevance = nn.Sequential(\n",
    "        nn.Linear(2, 1),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Linear(1,1),\n",
    "        )\n",
    "        \n",
    "        self.weight = nn.Sequential(\n",
    "        nn.Linear((n_neighbor+1)*2, n_feature),\n",
    "        nn.Sigmoid(),\n",
    "        nn.Linear(n_feature,n_feature),\n",
    "        nn.Sigmoid(),\n",
    "#         nn.Linear(n_feature,n_feature),\n",
    "#         nn.Sigmoid(),\n",
    "        nn.Linear(n_feature,2),\n",
    "        )\n",
    " \n",
    "    def forward(self, x, n_neighbor,verbose=False):\n",
    "        att = self.relevance(x)\n",
    "        \n",
    "        # relevance * flows\n",
    "        flow = att.mul(x[:,:1])\n",
    "        print(att.shape,x[:,:1].shape,flow.shape)\n",
    "        # reshape to get weighted avaerge from all neighbors at each time lag\n",
    "        flow = flow.view(int(flow.shape[0]/(n_neighbor+1)/2),(n_neighbor+1)*2)\n",
    "        flow = self.weight(x)\n",
    "        return flow\n",
    "\n",
    "def get_loss_and_metrics(model, batch, n_neighbor,criterion, device):\n",
    "  # Implement forward pass and loss calculation for one batch.\n",
    "  # Remember to move the batch to device.\n",
    "  # \n",
    "  # Return a tuple:\n",
    "  # - loss for the batch (Tensor)\n",
    "  # - number of correctly classified examples in the batch (Tensor)\n",
    "    data, target = batch[0], batch[1]\n",
    "    print(data.shape)\n",
    "    target[:int(target.shape[0]/(58*2*(n_neighbor+1)))]\n",
    "    data = torch.tensor(data, dtype=torch.float32)\n",
    "    target = torch.tensor(target, dtype=torch.float32)\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(data,n_neighbor)\n",
    "    loss = criterion(pred, target)\n",
    "    \n",
    "    \n",
    "    return (pred,target,loss)\n",
    "    \n",
    "def step(loss, optimizer):\n",
    "  # Implement backward pass and update.\n",
    "\n",
    "  # TODO\n",
    "    loss = loss\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "770d9b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PLZA']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_dist.loc[(nodes_dist['o']==station)&(nodes_dist['dist']<1)]['d'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6683f602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [26], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m     28\u001b[0m scaler\u001b[38;5;241m.\u001b[39mfit(fts_train)\n\u001b[0;32m---> 29\u001b[0m fts_train \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mtensor(scaler\u001b[38;5;241m.\u001b[39mtransform(fts_train))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "nodes_dist = pd.read_csv('nodes_dist.csv')\n",
    "# neighbors = list(nx.ego_graph(G, radius=1, n=station, distance='weight', center=False).nodes)\n",
    "neighbors = nodes_dist.loc[(nodes_dist['o']==station)&\\\n",
    "                           (nodes_dist['dist']<1)&\\\n",
    "                           (nodes_dist['o']!=nodes_dist['d'])]['d'].values.tolist()\n",
    "flows = ['incoming_flow-'+station,'outgoing_flow-'+station]+\\\n",
    "        ['incoming_flow-'+i for i in neighbors]+\\\n",
    "                  ['outgoing_flow-'+i for i in neighbors]\n",
    "\n",
    "subod = od[['Date','Hour']+flows]\n",
    "lags = list(range(1,4))+list(np.array(list(range(2,8)))*24)+list(np.array(list(range(2,8)))*24*7)\n",
    "for lag in lags:\n",
    "    temp = subod[flows].shift(lag)\n",
    "    temp.columns = [i+'-lag-'+str(lag) for i in flows]\n",
    "    subod = pd.concat([subod,temp],axis=1)\n",
    "subod = subod.dropna()\n",
    "print('here')\n",
    "subod_melt = subod.drop(columns=flows).melt(id_vars=['Date','Hour'])\n",
    "subod_melt = subod_melt.sort_values(by=['Date','Hour'])\n",
    "subod_melt['nearby'] = subod_melt['variable'].apply(lambda x:x.split('flow-')[1].split('-')[0])\n",
    "subod_melt['degree'] = subod_melt.apply(lambda x:nx.shortest_path_length(G,station,x['nearby']),axis=1)\n",
    "subod_melt['path_distance'] = subod_melt.apply(lambda x:nx.dijkstra_path_length(G,station,x['nearby']),axis=1)\n",
    "\n",
    "fts = subod_melt[['value','degree']]\n",
    "# 58 is number of lags, 2 is bidirectional\n",
    "fts_train = fts.iloc[:-24*7*len(lags)*2*(len(neighbors)+1),:].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(fts_train)\n",
    "fts_train = torch.tensor(scaler.transform(fts_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026ed6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                      | 0/50 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "G = nx.read_gpickle('graph.pickle')\n",
    "od = pd.read_csv('inoutwide.csv')\n",
    "pbar = tqdm(list(G.nodes))\n",
    "count = 0\n",
    "\n",
    "for station in pbar:\n",
    "    \n",
    "    neighbors = list(nx.ego_graph(G, radius=1, n=station, distance='weight', center=False).nodes)\n",
    "    flows = ['incoming_flow-'+station,'outgoing_flow-'+station]+\\\n",
    "            ['incoming_flow-'+i for i in neighbors]+\\\n",
    "                      ['outgoing_flow-'+i for i in neighbors]\n",
    "\n",
    "    subod = od[['Date','Hour']+flows]\n",
    "    lags = list(range(1,4))+list(np.array(list(range(2,8)))*24)+list(np.array(list(range(2,8)))*24*7)\n",
    "    for lag in lags:\n",
    "        temp = subod[flows].shift(lag)\n",
    "        temp.columns = [i+'-lag-'+str(lag) for i in flows]\n",
    "        subod = pd.concat([subod,temp],axis=1)\n",
    "    subod = subod.dropna()\n",
    "    print('here')\n",
    "    subod_melt = subod.drop(columns=flows).melt(id_vars=['Date','Hour'])\n",
    "    subod_melt = subod_melt.sort_values(by=['Date','Hour'])\n",
    "    subod_melt['nearby'] = subod_melt['variable'].apply(lambda x:x.split('flow-')[1].split('-')[0])\n",
    "    subod_melt['degree'] = subod_melt.apply(lambda x:nx.shortest_path_length(G,station,x['nearby']),axis=1)\n",
    "\n",
    "    fts = subod_melt[['value','degree']]\n",
    "    # 58 is number of lags, 2 is bidirectional\n",
    "    fts_train = fts.iloc[:-24*7*len(lags)*2*(len(neighbors)+1),:].values\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(fts_train)\n",
    "    fts_train = torch.tensor(scaler.transform(fts_train))\n",
    "\n",
    "    y = subod[['incoming_flow-'+station]]\n",
    "    y_train = y.iloc[:-24*7,:].values\n",
    "    y_scaler = StandardScaler()\n",
    "    y_scaler.fit(y_train)\n",
    "    y_train = torch.tensor(y_scaler.transform(y_train))\n",
    "    y_train = y_train.repeat(len(lags)*2*(len(neighbors)+1),1)\n",
    "    train_dataset = torch.utils.data.TensorDataset(fts_train,y_train)\n",
    " \n",
    "\n",
    "    fts_val = torch.tensor(scaler.transform(fts.iloc[-24*7*len(lags)*2*(len(neighbors)+1):,:].values)) \n",
    "    y_val = y.iloc[-24*7:,:].values\n",
    "    y_val = torch.tensor(y_val)\n",
    "    y_val = y_val.repeat(len(lags)*2*(len(neighbors)+1),1)\n",
    "    validation_dataset = torch.utils.data.TensorDataset(fts_val, y_val)\n",
    "    \n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "    N_EPOCHS = 1000\n",
    "    BATCH_SIZE = int(1*2*(len(neighbors)+1)*len(lags))\n",
    "    print('here')\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=len(validation_dataset),\n",
    "                                                        num_workers=0)\n",
    "    model = relevance_VNN(input_size=fts.shape[1],n_neighbor=len(neighbors),n_feature=64,output_size=2)\n",
    "    model = model.to(device)\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.00001) \n",
    "    train_losses = []\n",
    "\n",
    "    for i in range(N_EPOCHS): \n",
    "        total_train_loss = 0.0\n",
    "        model.train()\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            y_train,y_true,loss = get_loss_and_metrics(model, batch, len(neighbors),criterion, device)\n",
    "            step(loss.float(), optimizer)\n",
    "            total_train_loss += loss.item()\n",
    "        for batch in validation_dataloader:\n",
    "            with torch.no_grad(): \n",
    "                y_pred,y_true,loss = get_loss_and_metrics(model, batch,len(neighbors), criterion, device)\n",
    "                y_pred,y_true = y_pred.cpu().numpy(),y_true.cpu().numpy()\n",
    "                y_pred = y_scaler.inverse_transform(y_pred)\n",
    "                validation_mae = mean_absolute_error(y_true,y_pred)\n",
    "                validation_r2 = r2_score(y_true,y_pred)\n",
    "                validation_mape = masked_MAPE(y_true,y_pred)\n",
    "    \n",
    "        mean_train_loss = total_train_loss / len(train_dataloader)\n",
    "        pbar.set_description('train_loss:'+ str(round(mean_train_loss,4))+ \\\n",
    "                             ' validation_mae:'+ str(round(validation_mae,4)) +\\\n",
    "                            ' validation R2: '+ str(round(validation_r2,4)) +\\\n",
    "                            ' validation MAPE: '+ str(round(validation_mape,4)))\n",
    "    if count == 0: \n",
    "        y_true_total = y_true\n",
    "        y_pred_total = y_val\n",
    "    else:\n",
    "        y_true_total = np.concatenate([y_true_total,y_true])\n",
    "        y_pred_total = np.concatenate([y_pred_total,y_val])\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5d16f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(1*2*(len(neighbors)+1)*58)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5a04c6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "subod_melt = subod.drop(columns=['incoming_flow-'+station,\n",
    "                                 'outgoing_flow-'+station]).melt(id_vars=['Date','Hour'])\n",
    "subod_melt['nearby'] = subod_melt['variable'].apply(lambda x:x.split('flow-')[1].split('-')[0])\n",
    "subod_melt['degree'] = subod_melt.apply(lambda x:nx.shortest_path_length(G,station,x['nearby']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "e1692585",
   "metadata": {},
   "outputs": [],
   "source": [
    "station = '19TH'\n",
    "neighbors = list(nx.ego_graph(G, radius=1, n=station, distance='weight', center=False).nodes)\n",
    "flows = ['incoming_flow-'+station,'outgoing_flow-'+station]+\\\n",
    "    ['incoming_flow-'+i for i in neighbors]+\\\n",
    "              ['outgoing_flow-'+i for i in neighbors]\n",
    "\n",
    "subod = od[['Date','Hour']+flows]\n",
    "\n",
    "for lag in list(range(1,48))+list(np.array(list(range(3,8)))*24)+list(np.array(list(range(2,8)))*24*7):\n",
    "    temp = subod[flows].shift(lag)\n",
    "    temp.columns = [i+'-lag-'+str(lag) for i in flows]\n",
    "    subod = pd.concat([subod,temp],axis=1)\n",
    "subod = subod.dropna()\n",
    "\n",
    "subod_melt = subod.drop(columns=flows).melt(id_vars=['Date','Hour'])\n",
    "subod_melt['nearby'] = subod_melt['variable'].apply(lambda x:x.split('flow-')[1].split('-')[0])\n",
    "subod_melt['degree'] = subod_melt.apply(lambda x:nx.shortest_path_length(G,station,x['nearby']),axis=1)\n",
    "\n",
    "fts = subod_melt[['value','degree']]\n",
    "fts_train = fts.iloc[:-24*7*58*2*(len(neighbors)+1),:].values\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(fts_train)\n",
    "fts_train = torch.tensor(scaler.transform(fts_train))\n",
    "\n",
    "y = subod[['incoming_flow-'+station]]\n",
    "y_train = y.iloc[:-24*7,:].values\n",
    "y_scaler = StandardScaler()\n",
    "y_scaler.fit(y_train)\n",
    "y_train = torch.tensor(y_scaler.transform(y_train))\n",
    "y_train = y_train.repeat(58*2*(len(neighbors)+1),1)\n",
    "train_dataset = torch.utils.data.TensorDataset(fts_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "d97ab3ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1829088, 1])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "7acec5ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7100],\n",
       "        [-0.9838],\n",
       "        [-0.9916],\n",
       "        ...,\n",
       "        [-0.6866],\n",
       "        [-0.6084],\n",
       "        [-0.5927]], dtype=torch.float64)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "61e53611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Hour</th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "      <th>nearby</th>\n",
       "      <th>degree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-02-19</td>\n",
       "      <td>0</td>\n",
       "      <td>incoming_flow-19TH-lag-1</td>\n",
       "      <td>85.0</td>\n",
       "      <td>19TH</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5424</th>\n",
       "      <td>2022-02-19</td>\n",
       "      <td>0</td>\n",
       "      <td>outgoing_flow-19TH-lag-1</td>\n",
       "      <td>56.0</td>\n",
       "      <td>19TH</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10848</th>\n",
       "      <td>2022-02-19</td>\n",
       "      <td>0</td>\n",
       "      <td>incoming_flow-LAKE-lag-1</td>\n",
       "      <td>66.0</td>\n",
       "      <td>LAKE</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16272</th>\n",
       "      <td>2022-02-19</td>\n",
       "      <td>0</td>\n",
       "      <td>incoming_flow-12TH-lag-1</td>\n",
       "      <td>95.0</td>\n",
       "      <td>12TH</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21696</th>\n",
       "      <td>2022-02-19</td>\n",
       "      <td>0</td>\n",
       "      <td>outgoing_flow-LAKE-lag-1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>LAKE</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1865855</th>\n",
       "      <td>2022-10-02</td>\n",
       "      <td>23</td>\n",
       "      <td>outgoing_flow-19TH-lag-1176</td>\n",
       "      <td>22.0</td>\n",
       "      <td>19TH</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1871279</th>\n",
       "      <td>2022-10-02</td>\n",
       "      <td>23</td>\n",
       "      <td>incoming_flow-LAKE-lag-1176</td>\n",
       "      <td>38.0</td>\n",
       "      <td>LAKE</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1876703</th>\n",
       "      <td>2022-10-02</td>\n",
       "      <td>23</td>\n",
       "      <td>incoming_flow-12TH-lag-1176</td>\n",
       "      <td>52.0</td>\n",
       "      <td>12TH</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1882127</th>\n",
       "      <td>2022-10-02</td>\n",
       "      <td>23</td>\n",
       "      <td>outgoing_flow-LAKE-lag-1176</td>\n",
       "      <td>19.0</td>\n",
       "      <td>LAKE</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1887551</th>\n",
       "      <td>2022-10-02</td>\n",
       "      <td>23</td>\n",
       "      <td>outgoing_flow-12TH-lag-1176</td>\n",
       "      <td>20.0</td>\n",
       "      <td>12TH</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1887552 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Date  Hour                     variable  value nearby  degree\n",
       "0        2022-02-19     0     incoming_flow-19TH-lag-1   85.0   19TH       0\n",
       "5424     2022-02-19     0     outgoing_flow-19TH-lag-1   56.0   19TH       0\n",
       "10848    2022-02-19     0     incoming_flow-LAKE-lag-1   66.0   LAKE       2\n",
       "16272    2022-02-19     0     incoming_flow-12TH-lag-1   95.0   12TH       1\n",
       "21696    2022-02-19     0     outgoing_flow-LAKE-lag-1   19.0   LAKE       2\n",
       "...             ...   ...                          ...    ...    ...     ...\n",
       "1865855  2022-10-02    23  outgoing_flow-19TH-lag-1176   22.0   19TH       0\n",
       "1871279  2022-10-02    23  incoming_flow-LAKE-lag-1176   38.0   LAKE       2\n",
       "1876703  2022-10-02    23  incoming_flow-12TH-lag-1176   52.0   12TH       1\n",
       "1882127  2022-10-02    23  outgoing_flow-LAKE-lag-1176   19.0   LAKE       2\n",
       "1887551  2022-10-02    23  outgoing_flow-12TH-lag-1176   20.0   12TH       1\n",
       "\n",
       "[1887552 rows x 6 columns]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subod_melt.sort_values(by=['Date','Hour'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "d454bf4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1829088, 1])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e3883c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1850784, 2])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fts_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2a0aee0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "609696"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fts_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "3dafa3f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([609696, 2])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fts_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b4c9b992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5256.0"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "609696/116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec916de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2523ded5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(range(1,48))+list(np.array(list(range(3,8)))*24)+list(np.array(list(range(2,8)))*24*7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fd89c6ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118.0"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "640032/5424"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "36588e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "58*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "91cf8a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5424, 120)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subod.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f78084a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Hour</th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "      <th>nearby</th>\n",
       "      <th>degree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-02-19</td>\n",
       "      <td>0</td>\n",
       "      <td>incoming_flow-CAST</td>\n",
       "      <td>7.0</td>\n",
       "      <td>CAST</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-02-19</td>\n",
       "      <td>1</td>\n",
       "      <td>incoming_flow-CAST</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CAST</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-02-19</td>\n",
       "      <td>2</td>\n",
       "      <td>incoming_flow-CAST</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CAST</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-02-19</td>\n",
       "      <td>3</td>\n",
       "      <td>incoming_flow-CAST</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CAST</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-02-19</td>\n",
       "      <td>4</td>\n",
       "      <td>incoming_flow-CAST</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CAST</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640027</th>\n",
       "      <td>2022-10-02</td>\n",
       "      <td>19</td>\n",
       "      <td>outgoing_flow-CAST-lag-1176</td>\n",
       "      <td>14.0</td>\n",
       "      <td>CAST</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640028</th>\n",
       "      <td>2022-10-02</td>\n",
       "      <td>20</td>\n",
       "      <td>outgoing_flow-CAST-lag-1176</td>\n",
       "      <td>13.0</td>\n",
       "      <td>CAST</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640029</th>\n",
       "      <td>2022-10-02</td>\n",
       "      <td>21</td>\n",
       "      <td>outgoing_flow-CAST-lag-1176</td>\n",
       "      <td>10.0</td>\n",
       "      <td>CAST</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640030</th>\n",
       "      <td>2022-10-02</td>\n",
       "      <td>22</td>\n",
       "      <td>outgoing_flow-CAST-lag-1176</td>\n",
       "      <td>12.0</td>\n",
       "      <td>CAST</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640031</th>\n",
       "      <td>2022-10-02</td>\n",
       "      <td>23</td>\n",
       "      <td>outgoing_flow-CAST-lag-1176</td>\n",
       "      <td>3.0</td>\n",
       "      <td>CAST</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>640032 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Date  Hour                     variable  value nearby  degree\n",
       "0       2022-02-19     0           incoming_flow-CAST    7.0   CAST       0\n",
       "1       2022-02-19     1           incoming_flow-CAST    1.0   CAST       0\n",
       "2       2022-02-19     2           incoming_flow-CAST    0.0   CAST       0\n",
       "3       2022-02-19     3           incoming_flow-CAST    0.0   CAST       0\n",
       "4       2022-02-19     4           incoming_flow-CAST    0.0   CAST       0\n",
       "...            ...   ...                          ...    ...    ...     ...\n",
       "640027  2022-10-02    19  outgoing_flow-CAST-lag-1176   14.0   CAST       0\n",
       "640028  2022-10-02    20  outgoing_flow-CAST-lag-1176   13.0   CAST       0\n",
       "640029  2022-10-02    21  outgoing_flow-CAST-lag-1176   10.0   CAST       0\n",
       "640030  2022-10-02    22  outgoing_flow-CAST-lag-1176   12.0   CAST       0\n",
       "640031  2022-10-02    23  outgoing_flow-CAST-lag-1176    3.0   CAST       0\n",
       "\n",
       "[640032 rows x 6 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subod_melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "50d9d8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = list(nx.ego_graph(G, radius=1, n=station, distance='weight', center=False).nodes)\n",
    "flows = ['incoming_flow-'+station,'outgoing_flow-'+station]+\\\n",
    "    ['incoming_flow-'+i for i in neighbors]+\\\n",
    "              ['outgoing_flow-'+i for i in neighbors]\n",
    "\n",
    "subod = od[['Date','Hour']+flows]\n",
    "\n",
    "for lag in list(range(1,48))+list(np.array(list(range(3,8)))*24)+list(np.array(list(range(2,8)))*24*7):\n",
    "    temp = subod[flows].shift(lag)\n",
    "    temp.columns = [i+'-lag-'+str(lag) for i in flows]\n",
    "    subod = pd.concat([subod,temp],axis=1)\n",
    "subod = subod.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7b1e53a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Hour</th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "      <th>nearby</th>\n",
       "      <th>degree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-02-19</td>\n",
       "      <td>0</td>\n",
       "      <td>incoming_flow-CAST</td>\n",
       "      <td>7.0</td>\n",
       "      <td>CAST</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-02-19</td>\n",
       "      <td>1</td>\n",
       "      <td>incoming_flow-CAST</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CAST</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-02-19</td>\n",
       "      <td>2</td>\n",
       "      <td>incoming_flow-CAST</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CAST</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-02-19</td>\n",
       "      <td>3</td>\n",
       "      <td>incoming_flow-CAST</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CAST</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-02-19</td>\n",
       "      <td>4</td>\n",
       "      <td>incoming_flow-CAST</td>\n",
       "      <td>0.0</td>\n",
       "      <td>CAST</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Hour            variable  value nearby  degree\n",
       "0  2022-02-19     0  incoming_flow-CAST    7.0   CAST       0\n",
       "1  2022-02-19     1  incoming_flow-CAST    1.0   CAST       0\n",
       "2  2022-02-19     2  incoming_flow-CAST    0.0   CAST       0\n",
       "3  2022-02-19     3  incoming_flow-CAST    0.0   CAST       0\n",
       "4  2022-02-19     4  incoming_flow-CAST    0.0   CAST       0"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "32c6b99f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CAST'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'incoming_flow-CAST'.split('flow-')[1].split('-')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ae21fb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = subod.melt(id_vars=['Date','Hour'])\n",
    "test['nearby'] = test['variable'].apply(lambda x:x.split('flow-')[1].split('-')[0])\n",
    "test['degree'] = test.apply(lambda x:nx.shortest_path_length(G,station,x['nearby']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2298cada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1000, 1.2000, 1.3000],\n",
       "        [2.1000, 2.2000, 2.3000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.array([1.1,1.2,1.3,2.1,2.2,2.3]).reshape(-1,1)\n",
    "test = torch.tensor(test)\n",
    "test.view((2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0a2a144a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9783333554260818\n",
      "16.76745470988621\n",
      "32.61365709111786\n",
      "0.0006806279690539939\n",
      "0.5030006782152145\n"
     ]
    }
   ],
   "source": [
    "print(r2_score(y_test,y_pred_vnn))\n",
    "print(mean_absolute_error(y_test,y_pred_vnn))\n",
    "print(mean_squared_error(y_test,y_pred_vnn,squared=False))\n",
    "print(mean_squared_error(y_test,y_pred_vnn,squared=False)/np.mean(np.var(y_pred)))\n",
    "print(masked_MAPE(y_test,y_pred_vnn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "ce0cc751",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in validation_dataloader:\n",
    "    with torch.no_grad(): \n",
    "        y_pred,y_true,loss = get_loss_and_metrics(model, batch, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "788b0caa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[33., 11.],\n",
       "       [35., 33.],\n",
       "       [57.,  4.],\n",
       "       ...,\n",
       "       [22., 16.],\n",
       "       [ 7.,  5.],\n",
       "       [71., 31.]], dtype=float32)"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "66af217f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9097482689371424\n",
      "21.502193\n",
      "66.24005\n",
      "0.30688763313285594\n"
     ]
    }
   ],
   "source": [
    "y_true,y_pred = y_true.cpu().numpy(),y_pred.cpu().numpy()\n",
    "print(r2_score(y_true,y_pred))\n",
    "print(mean_absolute_error(y_true,y_pred))\n",
    "print(mean_squared_error(y_true,y_pred,squared=False))\n",
    "print(masked_MAPE(y_true,y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
